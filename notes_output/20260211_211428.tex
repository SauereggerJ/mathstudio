\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath, amssymb, amsfonts}
\begin{document}
\title{Linear Dependence of Vectors Shifted by w}
\maketitle
\setcounter{section}{11}
\section*{Problem 12}
Suppose $\{V_1, \ldots, V_m\}$ is linearly independent in $V$ and $w \in V$.

Prove that if $\{V_1+w, \ldots, V_m+w\}$ is linearly dependent, then $w \in \text{span}(\{V_1, \ldots, V_m\})$.

\subsection*{Solution:}

Assume $\{V_1+w, \ldots, V_m+w\}$ is linearly dependent. 

There exist $a_1, \ldots, a_m \in \mathbb{F}$, not all zero, such that:
$$
a_1(V_1+w) + \ldots + a_m(V_m+w) = 0$$

Rearranging the terms:
$$
(a_1 + \ldots + a_m)w = -a_1V_1 - \ldots - a_mV_m
$$

If $(a_1 + \ldots + a_m) = 0$, then $0 = -a_1V_1 - \ldots - a_mV_m$. 
This is impossible because $\{V_1, \ldots, V_m\}$ is linearly independent (since this implies $a_1 = a_2 = \ldots = a_m = 0$, which contradicts the assumption that not all $a_i$ are zero).

Therefore, $a_1 + \ldots + a_m \neq 0$. Let $b = a_1 + \ldots + a_m$. Since $b \neq 0$, we can divide by $b$:
$$w = -\frac{a_1}{b}V_1 - \ldots - \frac{a_m}{b}V_m$$

And thus $w \in \text{span}(\{V_1, \ldots, V_m\})$.
\n\section*{Recommended Reading}\n\begin{itemize}\n  \item \textbf{Linear algebra problem book} by Paul R. Halmos (Match: 0.69)\n  \item \textbf{Pr√ºfungstraining Lineare Algebra : Band I} by Thomas C. T. Michaels (Match: 0.68)\n  \item \textbf{Linear Algebra} by Seymour Lipschutz (Match: 0.68)\n\end{itemize}\n
\end{document}