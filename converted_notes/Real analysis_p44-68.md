---
title: Note from Real analysis (pp. 44-68)
author: Barry Simon
date: 2026-02-17 19:57
tags: [auto-note, Real analysis]
---



## Page 44

1.7. Some Linear Algebra 23

7. $\dim(\mathcal{P}_j)$ is called the algebraic multiplicity of $\lambda_j$. $\dim[\text{Ker}(N_j) \cap \text{Ran}(P_j)] = \dim\{x| Tx = \lambda x\}$ is the geometric multiplicity.

8. The $P_j$ are called spectral projections (although some reserve that for the case of self-adjoint matrices). They are also called eigenprojections and the $N$'s eigennilpotents.

We need (in Section 7.5) a result about the form of rank one projections in $\mathbb{C}^n$ and transposes in the Jordan normal form. For any matrix $A$, the transpose, $A^t$, is the matrix
$$(A^t)_{ij} = A_{ji} \quad (1.7.28)$$

**Proposition 1.7.7.** Let $A$ be an $n\times n$ complex matrix and
$$A = \sum_{k=1}^{m} (\lambda_k P_k + N_k) \quad (1.7.29)$$
its Jordan normal form. Then $A^t$ has the Jordan normal form
$$A^t = \sum_{k=1}^{m} \lambda_k P_k^t + N_k^t \quad (1.7.30)$$
If $\dim(P_{k_0})=1$ for some $k_0$ (which implies that $N_{k_0} = 0$), then there are vectors, $v, w$, obeying
$$Av = \lambda_{k_0}v, \quad A^t w = \lambda_{k_0}w \quad (1.7.31)$$
$$(P_{k_0})_{ij} = v_i w_j \quad (1.7.32)$$
$$\sum_{j} w_j v_j = 1 \quad (1.7.33)$$

*Proof.* $P_k P_\ell = \delta_{k\ell} P_k$ implies $(P^t_\ell)(P^t_k) = \delta_{k\ell} P^t_k$, $P_k N_k P_k = N_k$ implies $(P^t_k)(N^t_k)(P^t_k) = N^t_k$, and $(N_k)^m = 0$ implies $(N^t_k)^m = 0$. Thus, (1.7.30) is the Jordan normal form.

If $\dim(P_{k_0}) = 1$, (1.7.32) holds, where $v \in \text{Ran}(P)$ (so $Av = \lambda_{k_0}v$), and by looking at the adjoint, $w \in \text{Ran}(P^t)$ (so $A^t w = \lambda_{k_0}w$). $P^2 = P$ implies (1.7.33). $\square$

**Deﬁnition.** A real inner product space is a real vector space, $X$, and a map $\langle\cdot ,\cdot\rangle : X\times X\to\mathbb{R}$ so that

(i) $\mathbb{R}$ $\langle x,y\rangle =\langle y,x\rangle \quad (1.7.34)$
(ii) $\langle x,\cdot\rangle$ is linear in $\cdot$ for all $x\in X$.
(iii) $\langle x,x\rangle\geq 0$ and $\langle x,x\rangle = 0 \iff x = 0$.

## Page 45

{
  "markdown": "24 1. Preliminaries\nA (\\text{complex}) inner product space is a complex vector space, $X$, and a map $\\langle\\cdot,\\cdot\\rangle : X\\times X\\to\\mathbb{C}$ so that (ii), (iii) hold but (i)$_{\\mathbb{R}}$ is replaced by\n\n(i)$_{\\mathbb{C}}$ $\\langle x,y\\rangle =\\langle y,\\overline{x}\\rangle$ (1.7.35)\n\nRemarks. 1. Inner product spaces are discussed extensively in Chapter 3, especially the infinite-dimensional case.\n2. If (1.7.34) held, then $\\langle ix,ix\\rangle = i^2\\langle x,x\\rangle =-\\langle x,x\\rangle$, which is incompatible with (iii) if $\\dim(X) > 0$.\n3. See the Notes to Section 3.1 for a discussion of why we take linearity of $\\langle x,\\cdot\\rangle$ in the complex case; in the mathematics literature, linearity of $\\langle\\cdot,x\\rangle$ is more common.\n\n**Definition.** Let $X$ be a finite-dimensional (real or complex) inner product space. An operator $T : X \\to X$ is called self-adjoint if and only if for all $x,y \\in X$, we have\n\n$$\\langle x,Ty\\rangle =\\langle Tx,y\\rangle \\quad (1.7.36)$$\n\n**Lemma 1.7.8.** (a) Let $\\mu$ be an eigenvalue of $T$, a self-adjoint operator, that is, there is $x\\in X$ with $x\\neq 0$ and\n\n$$Tx = \\mu x \\quad (1.7.37)$$\n\nThen $\\mu$ is real.\n(b) If $x$ obeys (1.7.37) and $T$ is self-adjoint, then\n\n$$[x]^{\\perp} =\\{y|\\langle y,x\\rangle =0\\} \\quad (1.7.38)$$\n\nis an invariant subspace for $T$.\n\n**Proof.** (a) If $X$ is a real vector space, (a) is trivial since $\\mu x$ doesn’t make sense if $\\mu \\in \\mathbb{C}\\setminus\\mathbb{R}$. If $X$ is a complex space, (1.7.37) implies\n\n$$\\mu \\|x\\|^2 =\\langle x,Tx\\rangle =\\langle Tx,x\\rangle \\quad (\\text{by self-adjointness})$$\n$$=\\langle x,Tx\\rangle =\\overline{\\mu} \\|x\\|^2 \\quad (1.7.39)$$\n\nso $\\mu=\\overline{\\mu}$.\n(b) If $y\\in[x]^{\\perp}$, then $\\langle Ty,x\\rangle =\\langle y,Tx\\rangle = \\mu\\langle y,x\\rangle =0$, so $Ty \\in [x]^{\\perp}$. $\\square$\n\nOur second main theorem is:\n\n**Theorem 1.7.9 (Finite-Dimensional Spectral Theorem).** Let $T$ be a self-adjoint operator on a finite-dimensional vector space, $X$. Then $T$ has an orthonormal basis of eigenvectors, that is, if $n =\\dim(X)$, there exist $\{x_j\}_{j=1}^n \\in X^n$ and $\{\\mu_j\}_{j=1}^n \\in \\mathbb{K}^n$ so that\n\n(a) $Tx_j = \\mu_j x_j \\quad (1.7.40)$\n(b) $\\langle x_j,x_k\\rangle = \\delta_{jk}, \\quad j , k =1 ,\\dots,n \\quad (1.7.41)$\n\nLicensed to AMS.\nLicense or copyright restrictions may apply to redistribution; see http://www.ams.org/publications/ebooks/terms",
  "latex": "\\documentclass{article}\n\\usepackage{amsmath, amssymb}\n\\usepackage{amsthm}\n\\begin{document}\n\n\\noindent 24 1. Preliminaries\n\nA (\\text{complex}) inner product space is a complex vector space, $X$, and a\nmap\\$\\langle\\cdot,\\cdot\\rangle : X\\times X\\to\\mathbb{C}$ so that (ii), (iii) hold but (i)$_{\\mathbb{R}}$ is replaced by\n\n(i)$_{\\mathbb{C}}$ $\\langle x,y\\rangle =\\langle y,\\overline{x}\\rangle$ (1.7.35)\n\n\\medskip\nRemarks. 1. Inner product spaces are discussed extensively in Chapter 3,\nespecially the in\\textnormal{finite}-dimensional case.\n2. If (1.7.34) held, then $\\langle ix,ix\\rangle = i^2\\langle x,x\\rangle =-\\langle x,x\\rangle$, which is incompatible\nwith (iii) if $\\dim(X) > 0$.\n3. See the Notes to Section 3.1 for a discussion of why we take linearity of\n$\\langle x,\\cdot\\rangle$ in the complex case; in the mathematics literature, linearity of $\\langle\\cdot,x\\rangle$\nis more common.\n\n\\begin{definition} Let $X$ be a \\textnormal{finite}-dimensional (real or complex) inner product\nspace. An operator $T : X \\to X$ is called self-adjoint if and only if for all\n$x,y \\in X$, we have\n$$\\langle x,Ty\\rangle =\\langle Tx,y\\rangle \\quad (1.7.36)$$\n\\end{definition}\n\n\\begin{lemma}{1.7.8.} (a) Let $\\mu$ be an eigenvalue of $T$, a self-adjoint operator, that\nis, there is $x\\in X$ with $x\\neq 0$ and\n$$Tx = \\mu x \\quad (1.7.37)$$\nThen $\\mu$ is real.\n(b) If $x$ obeys (1.7.37) and $T$ is self-adjoint, then\n$$[x]^{\\perp} =\\{y|\\langle y,x\\rangle =0\\} \\quad (1.7.38)$$\nis an invariant subspace for $T$.\n\\end{lemma}\n\n\\begin{proof}\n(a) If $X$ is a real vector space, (a) is trivial since $\\mu x$ doesn\\textquotesingle{}t make\nsense if $\\mu \\in \\mathbb{C}\\setminus\\mathbb{R}$. If $X$ is a complex space, (1.7.37) implies\n$$\\mu \\|x\\|^2 =\\langle x,Tx\\rangle =\\langle Tx,x\\rangle \\quad (\\textnormal{by self-adjointness})$$\n$$=\\langle x,Tx\\rangle =\\overline{\\mu} \\|x\\|^2 \\quad (1.7.39)$$\nso $\\mu=\\overline{\\mu}$.\n(b) If $y\\in[x]^{\\perp}$, then $\\langle Ty,x\\rangle =\\langle y,Tx\\rangle = \\mu\\langle y,x\\rangle =0 $, so $Ty \\in [x]^{\\perp}$. \n\\end{proof}\n\\qedhere\n\nOur second main theorem is:\n\n\\begin{theorem}{1.7.9} (Finite-Dimensional Spectral Theorem). Let $T$ be a self-\nadjoint operator on a \\textnormal{finite}-dimensional vector space, $X$. Then $T$ has\nan orthonormal basis of eigenvectors, that is, if $n =\\dim(X)$, there exist\n$\{x_j\}_{j=1}^n \\in X^n$ and $\{\\mu_j\}_{j=1}^n \\in \\mathbb{K}^n$ so that\n\\begin{enumerate}\n\\item[(a)] $Tx_j = \\mu_j x_j \\quad (1.7.40)$\n\\item[(b)] $\\langle x_j,x_k\\rangle = \\delta_{jk}, \\quad j , k =1 ,\\dots,n \\quad (1.7.41)$\n\\end{enumerate}\n\\end{theorem}\n\n\\bigskip\nLicensed to AMS.\nLicense or copyright restrictions may apply to redistribution; see http://www.ams.org/publications/ebooks/terms\n\n\\end{document}"
}

## Page 46

{
  "markdown": "1.7. Some Linear Algebra 25\n\nSketch. The first key fact is that $T$ has at least one eigenvalue. In the complex case, we’ll see this below; it also follows from Theorem 1.7.6. In the real case, one can derive this from the complex case (Problem 13) or from a variational principle (Problem 14).\n\nOnce one has an eigenvalue, the proof follows by induction in $n = \\dim(X)$. $n = 1$ is trivial. For general $X$, find an eigenvector $x_n \\neq 0$ and eigenvalue $\\mu_n$. Replace $x_n$ by $x_n/\\|x_n\\|$ if need be to get a unit vector. $[x_n]^{\\perp}$ has dimension $n-1$ and is, by the lemma, invariant for $T$. Thus, by induction, $T \\upharpoonleft [x_n]^{\\perp}$ has an orthonormal basis of eigenvectors. Since $x_n$ is orthogonal to $[x_n]^{\\perp}$, we have that $\{x_1,\\dots,x_{n-1}\subset [x_n]^{\\perp}$, so (1.7.41) holds for $i,j = 1, \\dots, n$. $\\Box$\n\nRemarks. 1. The extensions of this result to infinite dimensions will be a major theme of Part 4. Section 3.2 of Part 4 discusses a class of infinite-dimensional operators (compact self-adjoint) for which the extension is direct—such operators have an orthonormal basis of eigenvectors. For general operators, the extension is more subtle, and this is a major theme of Chapter 5 of Part 4 for bounded self-adjoint operators and of Chapter 7 of Part 4 for unbounded self-adjoint operators.\n\n2. An operator on an inner product space is **unitary** if it is a linear bijection preserving the inner product. A finite matrix is **unitary** if it is a unitary operator as an operator on $\\mathbb{C}^{\\dim(X)}$; equivalently, if $\\sum_{j=1}^{\\dim(X)} u_{ij} \\bar{u}_{kj} = \\delta_{ik} = \\sum_{j=1}^{\\dim(X)} u_{ji} \\bar{u}_{jk}$. One way of stating the spectral theorem is that for any self-adjoint matrix, $M$, there is a unitary $U$ so that $U M U^{-1}$ is a diagonal matrix.\n\nAs we’ve seen, existence of eigenvalues for $K = \\mathbb{C}$ is critical. Let’s end by explaining an algebraic proof of this that we’ll extend in Problems 11 and 12 to get the full Jordan normal form. We need the strong form of the fundamental theorem of algebra (see Theorem 3.1.11 and Problem 2 of Section 3.1 of Part 2A): If $P(X)$ is a complex polynomial of degree $d$, $P(X)= a_d X^d + a_{d-1} X^{d-1} +\\cdots + a_0$ (1.7.42) with $\{a_j\}_{j=0}^d$ in $\\mathbb{C}$, then there exist $\{z_j\}_{j=1}^d$ in $\\mathbb{C}$, so that $P(X)$ factors $P(X)= a_d \\prod_{j=1}^d (X-z_j)$ (1.7.43)\n\nThe set of $m\\times m$ matrices is a vector space of dimension $m^2$ (a basis is the matrices with exactly one nonzero element which is 1). Thus, for any operator, $T$, on $\\mathbb{C}^m$, by the result in Problem 1, there is a polynomial of degree at most $m^2$ so $P(T) = 0$ (1.7.44)",
  "latex": "\\documentclass{article}\n\\usepackage{amsmath, amssymb, amsthm}\n\\begin{document}\n\n\\section*{1.7. Some Linear Algebra}\n\n\\noindent\\textbf{Sketch.} The first key fact is that $T$ has at least one eigenvalue. In the complex case, we’ll see this below; it also follows from Theorem 1.7.6. In the real case, one can derive this from the complex case (Problem 13) or from a variational principle (Problem 14).\n\nOnce one has an eigenvalue, the proof follows by induction in $n = \\dim(X)$. $n = 1$ is trivial. For general $X$, find an eigenvector $x_n \\neq 0$ and eigenvalue $\\mu_n$. Replace $x_n$ by $x_n/\\|x_n\\|$ if need be to get a unit vector. $[x_n]^{\\perp}$ has dimension $n-1$ and is, by the lemma, invariant for $T$. Thus, by induction, $T \\upharpoonleft [x_n]^{\\perp}$ has an orthonormal basis of eigenvectors. Since $x_n$ is orthogonal to $[x_n]^{\\perp}$, we have that $\\{x_1,\\dots,x_{n-1}\\} \\subset [x_n]^{\\perp}$, so (1.7.41) holds for $i,j = 1, \\dots, n$. \\hfill $\\Box$\n\n\\medskip\n\\noindent\\textbf{Remarks.} 1. The extensions of this result to infinite dimensions will be a major theme of Part 4. Section 3.2 of Part 4 discusses a class of infinite-dimensional operators (compact self-adjoint) for which the extension is direct—such operators have an orthonormal basis of eigenvectors. For general operators, the extension is more subtle, and this is a major theme of Chapter 5 of Part 4 for bounded self-adjoint operators and of Chapter 7 of Part 4 for unbounded self-adjoint operators.\n\n2. An operator on an inner product space is\\textbf{unitary} if it is a linear bijection preserving the inner product. A finite matrix is\\textbf{unitary} if it is a unitary operator as an operator on $\\mathbb{C}^{\\dim(X)}$; equivalently, if $\\sum_{j=1}^{\\dim(X)} u_{ij} \\overline{u_{kj}} = \\delta_{ik} = \\sum_{j=1}^{\\dim(X)} u_{ji} \\overline{u_{jk}}$. One way of stating the spectral theorem is that for any self-adjoint matrix, $M$, there is a unitary $U$ so that $U M U^{-1}$ is a diagonal matrix.\n\nAs we’ve seen, existence of eigenvalues for $K = \\mathbb{C}$ is critical. Let’s end by explaining an algebraic proof of this that we’ll extend in Problems 11 and 12 to get the full Jordan normal form. We need the strong form of the fundamental theorem of algebra (see Theorem 3.1.11 and Problem 2 of Section 3.1 of Part 2A): If $P(X)$ is a complex polynomial of degree $d$, $P(X)= a_d X^d + a_{d-1} X^{d-1} +\\cdots + a_0$ (1.7.42) with $\\{a_j\\}_{j=0}^d$ in $\\mathbb{C}$, then there exist $\\{z_j\\}_{j=1}^d$ in $\\mathbb{C}$, so that $P(X)$ factors $P(X)= a_d \\prod_{j=1}^d (X-z_j)$ (1.7.43)\n\nThe set of $m\\times m$ matrices is a vector space of dimension $m^2$ (a basis is the matrices with exactly one nonzero element which is 1). Thus, for any operator, $T$, on $\\mathbb{C}^m$, by the result in Problem 1, there is a polynomial of degree at most $m^2$ so $P(T) = 0$ (1.7.44)\n\n\\end{document}"
}

## Page 47

{
  "markdown": "26 1. Preliminaries\n\nLet $Q(X)$ be a polynomial of minimal degree with $Q(T) = 0$. By $(1.7.43)$,\n$$Q(T)=( T−z_1)Q_1(T) \quad (1.7.45)$$\nwhere $Q_1(T)$ is not identically zero since $\\deg(Q_1) < \\deg(Q)$ and $Q$ has\nminimal degree. If $y \\equiv Q_1(T)x \\neq 0 $, then $(T -z_1)y = 0$, that is, $z_1$ is an\neigenvalue of $T$ (and so are the other roots of the minimal polynomial).\n\nNotes and Historical Remarks. Linear algebra in the guise of solv-\ning simultaneous linear equations goes back to the Babylonians and Greeks.\nImportant advances using determinants were made in the eighteenth cen-\ntury. During the nineteenth century, matrices—often in the guise of qua-\ndratic forms (i.e., looking at $x \\mapsto \\sum_{k}^{i,j=1} x_i t_{ij} x_j$ in place of a linear\ntransformation)—began to be studied. Gauss [ 341] introduced the term\n`determinant`, although its modern usage is due to Cauchy. Important contri-\nbutions were made by Jacobi, Hermite, and especially Cayley and Sylvester.\nIn particular, Sylvester [898] introduced the term `matrix` (from Latin—one\nof its meanings is womb) in 1851 and an early classic was Cayley [176].\nIn 1888, Peano [705], as part of a book specifying a careful approach to\ncalculus, deﬁned vector spaces axiomatically.\n\nThe Jordan normal form appeared in an inﬂuential 1870 book of 660\npages by C. Jordan [454].\n\nThe (ﬁnite-dimensional) spectral theorem entered mathematics as a\nstatement about quadratic forms, not matrices. That quadratic forms in\ntwo variables could be written in oblique axes as those of a hyperbola or\nellipse appeared already in Descartes in 1637 [238] in his work on analytic\ngeometry, and the three-dimensional analog is in Euler [284]. In 1759, La-\ngrange [539] noted that quadratic forms in $n$ variables could be written as\nsums and diﬀerences of squares. Cauchy in 1829 [171] wrote out the result in\na way that made use of orthogonal changes of coordinates so that what we’d\ncall eigenvalues appeared, and his work is often viewed as the place where\nthe spectral theorem for ﬁnite-dimensional matrices was ﬁrst written down.\nIt was Sylvester [899] who wrote down the matrix version even having the\nresult that the diagonal matrix elements are the roots of $\\det(A-\\lambda I) = 0$.\nHermite [420] singled out what we now call Hermitian quadratic forms and\nproved their eigenvalues were real.\n\nProblems\n\n1. Let $y_1,\\dots,y_{\\ell+1}$ be $\\ell + 1$ vectors in $K^\\ell$. Suppose you know that any\n$x_1,\\dots,x_\\ell \\in K^{\\ell-1}$ are dependent.\n(a) If $y_{j\\ell} =0$ for $j =1 ,\\dots,\\ell + 1$ (the $\\ell$-th component), prove that the\n$y_j$'s are dependent.\n\nLicensed to AMS.\nLicense or copyright restrictions may apply to redistribution; see http://www.ams.org/publications/ebooks/terms",
  "latex": "\\documentclass{article}\n\\usepackage{amsmath, amssymb}\n\\usepackage{geometry}\n\\geometry{a4paper, margin=1in}\n\\begin{document}\n\n\\noindent 26 1. Preliminaries\n\nLet $Q(X)$ be a polynomial of minimal degree with $Q(T) = 0$. By $(1.7.43)$,\n$$\\begin{equation*}\nQ(T)=( T-z_1)Q_1(T) \\quad (1.7.45)\n\\end{equation*}$$ \nwhere $Q_1(T)$ is not identically zero since $\\deg(Q_1) < \\deg(Q)$ and $Q$ has\nminimal degree. If $y \\equiv Q_1(T)x\\neq 0 $, then $(T -z_1)y = 0$, that is, $z_1$ is an\neigenvalue of $T$ (and so are the other roots of the minimal polynomial).\n\nNotes and Historical Remarks. Linear algebra in the guise of solv-\ning simultaneous linear equations goes back to the Babylonians and Greeks. \nImportant advances using determinants were made in the eighteenth cen-\ntury. During the nineteenth century, matrices---often in the guise of qua-\ndratic forms (i.e., looking at $x \\mapsto \\sum_{k}^{i,j=1} x_i t_{ij} x_j$ in place of a linear\ntransformation)---began to be studied. Gauss [ 341] introduced the term\n`determinant`, although its modern usage is due to Cauchy. Important contri-\nbutions were made by Jacobi, Hermite, and especially Cayley and Sylvester.\nIn particular, Sylvester [898] introduced the term `matrix` (from Latin---one\nof its meanings is womb) in 1851 and an early classic was Cayley [176].\nIn 1888, Peano [705], as part of a book specifying a careful approach to\ncalculus, deﬁned vector spaces axiomatically.\nThe Jordan normal form appeared in an inﬂuential 1870 book of 660\npages by C. Jordan [454].\n\nThe (ﬁnite-dimensional) spectral theorem entered mathematics as a\nstatement about quadratic forms, not matrices. That quadratic forms in\ntwo variables could be written in oblique axes as those of a hyperbola or\nellipse appeared already in Descartes in 1637 [238] in his work on analytic\ngeometry, and the three-dimensional analog is in Euler [284]. In 1759, La-\ngrange [539] noted that quadratic forms in $n$ variables could be written as\nsumsand diﬀerencesof squares. Cauchy in 1829 [171] wrote out the result in\na way that made use of orthogonal changes of coordinates so that what we’d\ncall eigenvalues appeared, and his work is often viewed as the place where\nthe spectral theorem for ﬁnite-dimensional matrices was ﬁrst written down.\nIt was Sylvester [899] who wrote down the matrix version even having the\nresult that the diagonal matrix elements are the roots of $\\det(A-\\lambda I) = 0$.\nHermite [420] singled out what we now call Hermitian quadratic forms and\nproved their eigenvalues were real.\n\n\\section*{Problems}\n\n1. Let $y_1,\\dots,y_{\\ell+1}$ be $\\ell + 1$ vectors in $K^\\ell$. Suppose you know that any\n$x_1,\\dots,x_\\ell \\in K^{\\ell-1}$ are dependent.\n(a) If $y_{j\\ell} =0$ for $j =1 ,\\dots,\\ell + 1$ (the $\\ell$-th component), prove that the\n$y_j$'s are dependent.\n\n\\vspace{1em}\n\\noindent Licensed to AMS.\n\\noindent License or copyright restrictions may apply to redistribution; see http://www.ams.org/publications/ebooks/terms\n\n\\end{document}"
}

## Page 65

{
  "markdown": "The name weaker comes from the fact that if $x_n \\to x$ in $T_2$, then $x_n \\to x$ in $T_1$, but not necessarily vice-versa, so convergence in $T_2$ is a stronger assertion than convergence in $T_1$.\n\nNotice that if $\{T_\\alpha\\}_{\\alpha\\in I}$ is a family of topologies, $\\bigcap_{\\alpha\\in I} T_\\alpha$ is a topology that is weaker than each $T_\\alpha$. If $T_1$ is a weaker topology on $X$ than $T_2$, $f: X \\to Y$ and $Y$ has a fixed topology, and $f$ is continuous in $T_1$, it is continuous in $T_2$. Thus, there is a weakest topology in which $f$ is continuous, that is, $\\bigcap_\\alpha T_\\alpha$, where $\{T_\\alpha\}$ is the set of all topologies in which $f$ is continuous. This topology is $\{f^{-1}[A] | A \\in \\mathcal{T}_Y \\}$, the topology on $Y$.\n\nMore generally, if $F$ is a family of functions, $f_\\alpha: X \\to Y_\\alpha$ from a fixed $X$ to an $\\alpha$-dependent range $Y_\\alpha$ and each $Y_\\alpha$ has a given topology, there is a weakest topology on $X$ in which all $f_\\alpha$ are continuous. For there is at least one such topology, the discrete topology on $X$, and this weakest topology is $\\bigcap_\\beta T_\\beta$ over all topologies in which all $f_\\alpha$ are continuous. Alternatively, it is the topology with subbase $\{f_\\alpha^{-1}[A_\\alpha] | f_\\alpha \\in F, A_\\alpha$ is open in $Y_\\alpha\\}$. This is called the **weak topology** or **$F$-weak topology**.\n\nFinally, we discuss the notion of connectedness—actually, two notions: arcwise connected and (topologically) connected. Let $[0,1]$ denote $\{x\\in\\mathbb{R} | 0 \\le x \\le 1\\}$ with the usual (metric) topology. A **curve** (also called a **path** or an **arc**) in $X$, a topological space, is a continuous function, $\\gamma: [0,1] \\to X$. $\\gamma(0)$ and $\\gamma(1)$ are called its **endpoints**. We call $X$ **arcwise connected** if for all $x,y \\in X$, there is a curve whose endpoints are $x$ and $y$. In Problem 5, the reader will show in the general case that $x \\sim y$ if $x$ and $y$ are the endpoints of a curve defines an equivalence relation on $X$. The equivalence classes are called **arcwise connected components** of $X$. They are maximal arcwise connected subsets of $X$ (i.e., subsets that are arcwise connected in the relative topology).\n\nA **clopen set**, $A \\subset X$, a topological space, is a set that is both open and closed. $X$ is called **connected** if and only if the only clopen sets in $X$ are $\\emptyset$ and $X$. A subset $A \\subset X$ is called **connected** if and only if it is a connected space in the relative topology. Thus, a space, $X$, is connected if $A, B \\subset X$ with $A \\cap B = \\emptyset$ and $A \\cup B = X$ with both open (or both closed) implies either $A$ or $B$ is empty. It is in this way that connectedness arises most often in analysis. One proves $A=X$ by showing $A$ is clopen in $X$ and nonempty and $X$ is connected.\n\n**Example 2.1.12.** Let $a < b$ in $\\mathbb{R}$. We claim $(a,b)$ is connected. For suppose $A \\cap B = \\emptyset$, $A \\cup B = (a,b)$ and both are open and closed in the relative topology. Suppose $A \\ne \\emptyset$ and $c \\in A$. Let $B^- = \\{x\\in B | x < c \\}$. If $B^-$ is nonempty, let $x_+ = \\sup\\{x \\in B^- \\}$. Then there exist $x_n \\in B$ with $x_+ - 1/n < x_n \\le x_+$. Thus, $x_n \\to x_+$, so $x_+ \\in B^-$ since $B$ is closed (and $x_+ \\ne c \\notin B$). But since $B^- = B \\cap (a,c)$ is open, $B^-$ has points",
  "latex": "\\documentclass{article}\n\\usepackage{amsmath, amssymb, amsthm}\n\\begin{document}\n\n\\noindent 44 2. Topological Spaces\n\nThe name weaker comes from the fact that if $x_n \\to x$ in $T_2$, then $x_n \\to x$ in $T_1$, but not necessarily vice-versa, so convergence in $T_2$ is a stronger assertion than convergence in $T_1$.\n\nNotice that if $\\{T_\\alpha\\}_{\\alpha\\in I}$ is a family of topologies, $\\bigcap_{\\alpha\\in I} T_\\alpha$ is a topology that is weaker than each $T_\\alpha$. If $T_1$ is a weaker topology on $X$ than $T_2$, $f: X \\to Y$ and $Y$ has a fixed topology, and $f$ is continuous in $T_1$, it is continuous in $T_2$. Thus, there is a weakest topology in which $f$ is continuous, that is, $\\bigcap_\\alpha T_\\alpha$, where $\\{T_\\alpha\\}$ is the set of all topologies in which $f$ is continuous. This topology is $\\{f^{-1}[A]\\mid A\\in\\mathcal{T}_Y \\}$, the topology on $Y$.\n\nMore generally, if $F$ is a family of functions, $f_\\alpha: X \\to Y_\\alpha$ from a fixed $X$ to an $\\alpha$-dependent range $Y_\\alpha$ and each $Y_\\alpha$ has a given topology, there is a weakest topology on $X$ in which all $f_\\alpha$ are continuous. For there is at least one such topology, the discrete topology on $X$, and this weakest topology is $\\bigcap_\\beta T_\\beta$ over all topologies in which all $f_\\alpha$ are continuous. Alternatively, it is the topology with subbase $\\{f_\\alpha^{-1}[A_\\alpha]\\mid f_\\alpha\\in F, A_\\alpha$ is open in $Y_\\alpha\\}$. This is called the **weak topology** or **$F$-weak topology**.\n\nFinally, we discuss the notion of connectedness---actually, two notions: arcwise connected and (topologically) connected. Let $[0,1]$ denote $\\{x\\in\\mathbb{R}\\mid 0 \\le x \\le 1\\}$ with the usual (metric) topology. A **curve** (also called a **path** or an **arc**) in $X$, a topological space, is a continuous function, $\\gamma:[ 0,1]\\to X$. $\\gamma(0)$ and $\\gamma(1)$ are called its **endpoints**. We call $X$ **arcwise connected** if for all $x,y \\in X$, there is a curve whose endpoints are $x$ and $y$. In Problem 5, the reader will show in the general case that $x \\sim y$ if $x$ and $y$ are the endpoints of a curve defines an equivalence relation on $X$. The equivalence classes are called **arcwise connected components** of $X$. They are maximal arcwise connected subsets of $X$ (i.e., subsets that are arcwise connected in the relative topology).\n\nA **clopen set**, $A \\subset X$, a topological space, is a set that is both open and closed. $X$ is called **connected** if and only if the only clopen sets in $X$ are $\\emptyset$ and $X$. A subset $A \\subset X$ is called **connected** if and only if it is a connected space in the relative topology. Thus, a space, $X$, is connected if $A, B \\subset X$ with $A \\cap B = \\emptyset$ and $A \\cup B = X$ with both open (or both closed) implies either $A$ or $B$ is empty. It is in this way that connectedness arises most often in analysis. One proves $A=X$ by showing $A$ is clopen in $X$ and nonempty and $X$ is connected.\n\n\\noindent\\textbf{Example 2.1.12.} Let $a<b$ in $\\mathbb{R}$. We claim $(a,b)$ is connected. For suppose $A\\cap B = \\emptyset$, $A\\cup B =(a,b)$ and both are open and closed in the relative topology. Suppose $A\\ne\\emptyset$ and $c\\in A$. Let $B^- =\\{x\\in B \\mid x<c \\}$. If $B^-$ is nonempty, let $x_+ =\\sup\\{x \\in B^- \\}$. Then there exist $x_n \\in B$ with $x_+ - 1/n < x_n \\le x_+$. Thus, $x_n \\to x_+$, so $x_+ \\in B^-$ since $B$ is closed (and $x_+ \\ne c \\notin B$). But since $B^- = B\\cap(a,c)$ is open, $B^-$ has points\n\n\\end{document}"
}

## Page 66

in $(x^+, c)$, contradicting that $x^+$ is the sup. Thus, $B^- = \emptyset$. Similarly, $B^+ =\{x\in B\mid x>c \}$ is empty, so $B=\emptyset$. \hfill \(\*\)

Here are some of the most significant properties associated to connectedness:

**Theorem 2.1.13.** (a) If $A\subset X$ is connected, so is $\overline{A}$.
(b) If $f: X\to Y$ is continuous and onto and $X$ is connected, so is $Y$.
(c) Any arcwise connected space is connected.
(d) If $\{A_\alpha\}_{\alpha\in I}$ is a collection of connected subsets of $X$, a topological space, so that for all $\alpha,\beta\in I$, $A_\alpha\cap A_\beta\neq\emptyset$, then $\cup_{\alpha\in I}A_\alpha$ is connected.
(e) If $x\in X$, a topological space, there is a unique connected $A\subset X$ with $x\in A$ that is maximal among all connected subsets of $X$ containing $x$. This $A$ is closed.

**Remarks.** 1. We’ll see below (Example 2.1.15) that connected spaces need not be arcwise connected.
2. The maximal connected $A \ni x$ is called a **connected component** of $X$. Lying in the same connected component is an equivalence relation.
3. Connected components need not be open in $X$ (see Example 2.1.14).

**Proof.** (a) Suppose $\overline{A} = B\cup C$ with $B\cap C =\emptyset$ and $B,C$ clopen in $\overline{A}$. Then $B\cap A$ and $C\cap A$ are clopen in $A$ so, since $A$ is connected, one is empty, that is, we can suppose $A\subset B$. Since $B$ is closed in $\overline{A}$, it is closed in $X$, and thus, $\overline{A}\subset B$ since $\overline{A}$ is the smallest closed set containing $A$. Thus, $\overline{A} = B$ and $C =\emptyset$.
(b) If $B,C$ are clopen in $Y$, $B\cap C =\emptyset$, and $B\neq\emptyset$, then $f^{-1}[B]\cap f^{-1}[C]= \emptyset$ and both sets are clopen in $X$. Since $X$ is connected, one can see $f^{-1}[C]$ must be empty. Since $f$ is onto $Y$, $C$ must be empty.
(c) Suppose $X$ is arcwise connected, $B,C$ are clopen in $X$ and disjoint and nonempty and $B\cup C = X$. Pick $x\in B, y \in C$, and $\gamma:[0,1] \to X$ with $\gamma(0) =x, \gamma(1) =y$. $\gamma^{-1}[B], \gamma^{-1}[C]$ are clopen and disjoint and cover $[0,1]$. But $0\in\gamma^{-1}[B], 1\in\gamma^{-1}[C]$, and $[0,1]$ is connected. This is a contradiction, so one of $B$ or $C$ is empty.
(d) Let $B,C$ be clopen in $Y =\cup_\alpha A_\alpha$ and disjoint with $B\cup C = Y$. Then for each $\alpha$, $B\cap A_\alpha, C\cap A_\alpha$ are clopen in $A_\alpha$ and disjoint, so since $A_\alpha$ is connected, for each $A_\alpha$, either $A_\alpha\subset B$ or $A_\alpha\subset C$. Suppose that for some $\alpha,\beta$, $A_\alpha\subset B, A_\beta\subset C$. Then, since $B\cap C =\emptyset$, $A_\alpha\cap A_\beta=\emptyset$, contrary to hypothesis. We conclude that either all $A_\alpha\subset B$ (so $C =\emptyset$) or vice-versa.
(e) Let $\{A_\alpha\}_{\alpha\in I}$ be a labeling of all connected subsets containing $x$. Clearly, $A_\alpha\cap A_\beta\neq\emptyset$ since it contains $x$. Thus, $A\equiv\cup A_\alpha$ is connected by (d) and is

## Page 67

{
  "markdown": "the largest connected set containing $x$. Since $\\overline{A}$ is also connected (by (a)), $A = \\overline{A}$. $\\square$\n\n**Example 2.1.14.** Let $X = [0,1] \\cap \\mathbb{Q}$, the rationals within $[0,1]$ with the topology induced by the standard topology on $[0,1]$. If $\\alpha,\\beta \\in A \\subset X$ are distinct, and $\\gamma$ is an irrational number between $\\alpha$ and $\\beta$, $(-\\infty,\\gamma) \\cap A$ and $(\\gamma,\\infty) \\cap A$ are disjoint, clopen, cover $A$, and are not empty. Thus, $A$ is not connected. That implies the only connected subsets of $X$ are single points. Such a space is called **totally disconnected**. Notice that components are not open in $X$.\n\nSection 5.4 will construct a connected subset, $A$, of $\\mathbb{R}^2$ so that for one special point, $x_\\infty$, $A\\{x_\\infty\}$ is totally disconnected (see Example 5.4.23). $\\square$\n\n**Example 2.1.15.** Let\n$$X = \\{(x,y) \\in \\mathbb{R}^2 \\mid x \\in (0,1), y = \\sin\\left(\\frac{1}{x}\\right)\\} \\cup \\{(x,y) \\mid x = 0 , |y| \\leq 1\\} \\quad (2.1.15)$$\nwith the topology induced by the standard one on $\\mathbb{R}^2$. We’ll call the right side of (2.1.15) $X_1 \\cup X_2$ (see Figure 2.1.1). $X_1$ is connected and even arcwise connected, since if $x_1 < x_2$, $(x_1,y_1),(x_2,y_2) \\in X$, then $\\gamma(t) = ((1 -t)x_1 + tx_2, \\sin(1/((1-t)x_1 + tx_2)))$ is a curve between the two points. Since $X_1$ is dense in $X$ (Problem 6(a)), $X$ is connected by Theorem 2.1.13(a). On the other hand (Problem 6(b)), $X_1$ and $X_2$ are the arcwise connected components, that is, $X$ is connected but not arcwise connected. $\\square$\n\n(Figure 2.1.1 caption: A connected but not arcwise connected set.)\n\nWhile, in general, connected sets need not be arcwise connected, there is a case in which one can prove they are.\n\n**Definition.** We say a topological space, $X$, is **locally arcwise connected** if any $x \\in X$ has an arcwise connected neighborhood.\n\n**Theorem 2.1.16.** If $X$, a topological space, is connected and locally arcwise connected, it is arcwise connected.",
  "latex": "\\documentclass{article}\n\\usepackage{amsmath, amssymb}\n\\usepackage{graphicx}\n\\begin{document}\n\n46 2. Topological Spaces\n\nthe largest connected set containing $x$. Since $\\overline{A}$ is also connected (by (a)),\n$A = \\overline{A}$. $\\square$\n\n\\textbf{Example 2.1.14.} Let $X = [0,1]\\cap\\mathbb{Q}$, the rationals within $[0,1]$ with the\ntopology induced by the standard topology on $[0,1]$. If $\\alpha,\\beta\\in A\\subset X$ are\ndistinct, and $\\gamma$ is an irrational number between $\\alpha$ and $\\beta$, $(-\\infty,\\gamma)\\cap A$ and\n$(\\gamma,\\infty)\\cap A$ are disjoint, clopen, cover $A$, and are not empty. Thus,$A$ is not\nconnected. That implies the only connected subsets of $X$ are single points.\nSuch a space is called \\textbf{totally disconnected}. Notice that components are not open in $X$.\n\nSection 5.4 will construct a connected subset, $A$, of $\\mathbb{R}^2$ so that for one\nspecial point, $x_\\infty$, $A\\{x_\\infty\}$ is totally disconnected (see Example 5.4.23).$\\square$\n\n\\textbf{Example 2.1.15.} Let\n$$X = \\{(x,y)\\in\\mathbb{R}^2 \\mid x\\in(0,1), y = \\sin\\left(\\frac{1}{x}\\right)\\} \\cup \\{(x,y)\\mid x = 0 ,|y|\\leq 1\\} \\quad (2.1.15)$$\nwith the topology induced by the standard one on $\\mathbb{R}^2$. We’ll call the\nright side of (2.1.15) $X_1 \\cup X_2$ (see Figure 2.1.1). $X_1$ is connected and\neven arcwise connected, since if $x_1 < x_2,(x_1,y_1),(x_2,y_2)\\in X$, then\n$\\gamma(t) = ((1 -t)x_1 + tx_2,\\sin\\left(1/((1-t)x_1 + tx_2)\\right))$ is a curve between the\ntwo points. Since $X_1$ is dense in $X$ (Problem 6(a)), $X$ is connected by\nTheorem 2.1.13(a). On the other hand (Problem 6(b)), $X_1$ and $X_2$ are\nthe arcwise connected components, that is,$X$ is connected but not arcwise\nconnected. $\\square$\n\n\\begin{figure}[h!]\n\\centering\n% Placeholder for the figure content based on description\n\\begin{tikzpicture}[scale=3]\n\\draw[gray, thick] (0,0) -- (1,0);\n\\draw[->, gray] (0,-1.1) -- (0,1.1) node[above] {};\n\\draw[->, gray] (-0.1,0) -- (1.1,0) node[right] {};\n\\draw[gray] (0.2,0) -- (0.2,-0.02) node[below] {$0.2$}; \n\\draw[gray] (0.4,0) -- (0.4,-0.02) node[below] {$0.4$}; \n\\draw[gray] (0.6,0) -- (0.6,-0.02) node[below] {$0.6$}; \n\\draw[gray] (0.8,0) -- (0.8,-0.02) node[below] {$0.8$}; \n\\draw[gray] (1.0,0) -- (1.0,-0.02) node[below] {$1.0$}; \n\\draw[gray] (0,-1.0) -- (0.02,-1.0) node[left] {$-1.0$}; \n\\draw[gray] (0,-0.5) -- (0.02,-0.5) node[left] {$-0.5$}; \n\\draw[gray] (0,0) -- (0.02,0) node[left] {$0.0$}; \n\\draw[gray] (0,0.5) -- (0.02,0.5) node[left] {$0.5$}; \n\\draw[gray] (0,1.0) -- (0.02,1.0) node[left] {$1.0$}; \n\n% Plot the curve y = sin(1/x) for x in (0, 1)\n\\draw[blue, thick] plot [domain=0.05:1, samples=100] (\n  \\x, {sin(1/\\x r) * 1}\n);\n\n% Plot the segment X2 on the y-axis\n\\draw[red, thick] (0, -1) -- (0, 1);\n\n% Highlighting the curve X1 (excluding endpoints for clarity if needed, but the plot covers it)\n% Let's assume the blue line represents X1\n\n\\end{tikzpicture}\n\\caption{A connected but not arcwise connected set. (Figure schematic)} \n\\label{fig:2.1.1}\n\\end{figure}\n\nWhile, in general, connected sets need not be arcwise connected, there\nis a case in which one can prove they are.\n\n\\textbf{Definition.} We say a topological space, $X$, is \\textbf{locally arcwise connected} if\nany $x\\in X$ has an arcwise connected neighborhood.\n\n\\textbf{Theorem 2.1.16.} If $X$, a topological space, is connected and locally arcwise\nconnected, it is arcwise connected.\n\n\\end{document}"
}

## Page 68

2.1. Lots of Deﬁnitions 47

**Proof.** Let $x \in X$ and $A = \{y \in X \mid \text{there is a curve } \gamma \text{ from } x \text{ to } y\}$. Obviously, $A$ is nonempty. If we prove it is clopen, then $A = X$ and $X$ is arcwise connected.

Let $y \in A$. By hypothesis, there is an arcwise connected neighborhood $N$ of $y$. If $z \in N$, we can ﬁnd a path from $x$ to $y$ and follow it by a path from $y$ to $z$ and so see that $N \subset A$, that is, $A$ is open.

Let $y \in \bar{A} \setminus A$. Then $y$ is an accumulation point of $A$. So if $N$ is an arcwise connected neighborhood of $y$, we can ﬁnd $z \in A \cap N$. Thus, we can follow a path from $x$ to $z$ by one from $z$ to $y$ and so see $y \in A$ after all. Thus, $A$ is closed. $\square$

In $\mathbb{R}^n$, $\{x \mid \|x - x_0\| < \rho\}$ is arcwise connected, since for $x, y$ in that ball, $\gamma(t) = (1-t)x + ty$ is in the ball. Thus, open sets in $\mathbb{R}^n$ are locally arcwise connected, and we see

**Corollary 2.1.17.** Open connected subsets of $\mathbb{R}^n$ are arcwise connected.

**Notes and Historical Remarks.** Topology is a vast subject and includes areas like combinatorial topology (going back to Euler) and algebraic topology (whose father was Poincar´e). Point set topology, our subject in this chapter, had its roots in the work of Cauchy and Weierstrass, discussed in the Notes to Section 3.5, and of Cantor and Baire, discussed in the Notes to Sections 4.2 and 4.3. There were two motivating aspects: First, the attempts to make sense of surfaces, especially Riemann surfaces, that led to the modern deﬁnition of manifold. Second, the work of Fredholm [324] on integral equations, that motivated both Hilbert’s work that led to the theory of Hilbert spaces, and attempts to discuss convergence on inﬁnite-dimensional spaces that were concerns of Fr´echet and Riesz.

After several shorter papers, Maurice R´en´e Fr´echet (1878–1973) published a comprehensive theory of convergence in his 1906 thesis [315] that essentially deﬁned axiomatically what we now call metric spaces. Fr´echet had a weaker condition than the triangle inequality, but mentioned the triangle inequality as a special case. It was F. Riesz (see below) who emphasized the triangle inequality. The ﬁrst half of Fr´echet’s thesis also had an attempt at convergence without a metric. Fr´echet deﬁned Cauchy sequences and, without the name, completeness. Most importantly, he was the ﬁrst to try to discuss convergence on abstract sets of points rather than $\mathbb{R}^n$ or explicit spaces of functions.

Between the work of Fr´echet and Hausdorﬀ, F. Riesz [777, 778] developed some ideas of point set topology and H. Weyl [982] wrote a path-breaking book on Riemann surfaces which emphasized the role of neighborhoods. In 1914, Felix Hausdorﬀ gave birth to the modern theory of point

Licensed to AMS.
License or copyright restrictions may apply to redistribution; see http://www.ams.org/publications/ebooks/terms